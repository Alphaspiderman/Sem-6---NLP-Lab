# Report on News Article Classification and Summarization

## Abstract
In the digital age, the overwhelming influx of news articles necessitates efficient methods for classification and summarization. This report outlines a systematic approach to automatically classify and summarize news articles into predefined categories such as "Politics," "Sports," and "Technology." The process involves data collection, preprocessing, exploratory data analysis, text classification using various machine learning models, and summarization techniques. The goal is to enhance organization and facilitate quick reader consumption of news content. The report also discusses the advantages and disadvantages of the implemented methods.

## Steps

1. **Collect and Preprocess News Article Data**
   - Utilize publicly available datasets such as the BBC News Classification Dataset, AG News, or Reuters-21578.

2. **Preprocess the Text**
   - Remove special characters, stopwords, numbers, and URLs.
   - Tokenize and lemmatize the text.

3. **Analyze the Data**
   - Analyze the distribution of articles across categories.
   - Calculate average word count and sentence length of articles.
   - Visualize word frequency using bar charts or word clouds for each category.

4. **Text Classification**
   - Classify articles into predefined categories using various models:
     - Traditional: Naïve Bayes, Support Vector Machine (SVM)
     - Deep Learning: Long Short-Term Memory (LSTM), Convolutional Neural Networks (CNNs)

5. **Evaluate the Models**
   - Split the dataset into training and testing sets.
   - Evaluate the models using precision, recall, and F1-score.

6. **Text Summarization**
   - Implement extractive summarization techniques.
   - Implement abstractive summarization techniques.
   - Compare the quality of summaries generated by both approaches.

## Python Code

```python
# Import necessary libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D
from keras.preprocessing.sequence import pad_sequences
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Step 1: Load dataset
data = pd.read_csv('news_articles.csv')  # Replace with actual dataset path

# Step 2: Preprocess the text
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\@\w+|\#','', text)  # Remove mentions and hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = text.lower()  # Convert to lowercase
    tokens = word_tokenize(text)  # Tokenize
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    return ' '.join(tokens)

data['cleaned_text'] = data['article_text'].apply(preprocess_text)

# Step 3: Analyze the data
category_counts = data['category'].value_counts()
plt.bar(category_counts.index, category_counts.values)
plt.title('Distribution of Articles Across Categories')
plt.xlabel('Category')
plt.ylabel('Number of Articles')
plt.show()

# Step 4: Text Classification
X = data['cleaned_text']
y = data['category']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Naive Bayes Classifier
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

nb_model = MultinomialNB()
nb_model.fit(X_train_vectorized, y_train)
y_pred_nb = nb_model.predict(X_test_vectorized)

print("Naive Bayes Classification Report:")
print(classification_report(y_test, y_pred_nb))

# SVM Classifier
svm_model = SVC()
svm_model.fit(X_train_vectorized, y_train)
y_pred_svm = svm_model.predict(X_test_vectorized)

print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))

# Step 5: Text Summarization (Extractive)
from gensim.summarization import summarize

data['summary'] = data['article_text'].apply(lambda x: summarize(x, ratio=0.1))

# Display summaries
```python
# Display summaries for the first few articles
for i in range(5):
    print(f"Original Article {i+1}:\n{data['article_text'].iloc[i]}\n")
    print(f"Extractive Summary {i+1}:\n{data['summary'].iloc[i]}\n")
    print("="*80)

# Step 6: Abstractive Summarization (using a simple LSTM model)
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# Prepare data for LSTM
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['cleaned_text'])
total_words = len(tokenizer.word_index) + 1

input_sequences = []
for line in data['cleaned_text']:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i + 1]
        input_sequences.append(n_gram_sequence)

# Pad sequences
max_sequence_length = max([len(x) for x in input_sequences])
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')

# Create predictors and label
X_lstm, y_lstm = input_sequences[:, :-1], input_sequences[:, -1]
y_lstm = np.array([np.zeros(total_words) for _ in range(len(y_lstm))])
for i, word_index in enumerate(y_lstm):
    word_index[y_lstm[i]] = 1

# Build LSTM model
model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100))
model.add(Dense(total_words, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_lstm, y_lstm, epochs=10, batch_size=64)

# Generate summaries using the trained LSTM model (this is a simplified example)
def generate_summary(text):
    token_list = tokenizer.texts_to_sequences([text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')
    predicted = model.predict(token_list, verbose=0)
    return ' '.join([tokenizer.index_word[np.argmax(pred)] for pred in predicted])

# Display LSTM generated summaries for the first few articles
for i in range(5):
    print(f"Original Article {i+1}:\n{data['article_text'].iloc[i]}\n")
    print(f"Abstractive Summary {i+1}:\n{generate_summary(data['cleaned_text'].iloc[i])}\n")
    print("="*80)
```

## Advantages and Disadvantages

### Advantages
1. **Automation**: The system automates the classification and summarization of news articles, saving time and effort for news agencies.
2. **Improved Organization**: By categorizing articles, readers can easily find content relevant to their interests.
3. **Concise Information**: Summarization provides readers with quick insights into articles, enhancing user experience.
4. **Scalability**: The system can handle large volumes of articles, making it suitable for news agencies with extensive content.

### Disadvantages
1. **Model Limitations**: Traditional models like Naïve Bayes and SVM may not capture complex patterns in text as effectively as deep learning models.
2. **Data Dependency**: The performance of classification and summarization heavily relies on the quality and quantity of the training data.
3. **Summarization Quality**: Extractive summarization may not always capture the essence of the article, while abstractive summarization can generate incoherent or irrelevant summaries.
4. **Computational Resources**: Deep learning models, especially LSTMs, require significant computational resources and time for training.

## Conclusion
The implementation of a news article classification and summarization system demonstrates the potential of machine learning and natural language processing in enhancing the organization and accessibility of news content. By leveraging various models and techniques, the system can effectively classify articles and generate concise summaries, ultimately improving the reader's experience. However, careful consideration of model selection, data quality, and computational resources is essential for optimal performance. Future work may involve refining the models and exploring advanced techniques to further enhance summarization quality.